{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document QA\n",
    "\n",
    " This is the implementation of paper [Simple and Effective Multi-Paragraph Reading Comprehension](https://arxiv.org/pdf/1710.10723.pdf) (Clark et. al., 2017).\n",
    "- Code is adapted from [https://github.com/allenai/document-qa](https://github.com/allenai/document-qa)\n",
    "- Please contact the original authors for questions and suggestions. \n",
    "\n",
    "This model, by [Clark et. al., 2017](https://arxiv.org/pdf/1710.10723.pdf), considers the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Here the authors proposed a solution that trains models to produce well calibrated confidence scores for their results on individual paragraphs. This model sample multiple paragraphs from the documents during training and use a shared normalization training objective that encourages the model to produce globally correct output. Next, this method is combined with a state-of-the-art pipeline for training models on document QA data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Setup\n",
    "### Dependencies\n",
    "We require python >= 3.5, tensorflow 1.2, and a handful of other supporting libraries. \n",
    "Tensorflow should be installed separately following the docs. To install the other dependencies use the commands below from a terminal in the linux DLVM first.\n",
    "\n",
    "```\n",
    "\n",
    "conda create --name <myenv>\n",
    "source activate <myenv>\n",
    "cd /home/<user>/notebooks\n",
    "git clone https://github.com/antriv/Transfer_Learning_Text.git\n",
    "cd Transfer_Learning_Text/Transfer_Learning/document-qa/\n",
    "pip install -r requirements.txt\n",
    "pip install tensorflow-gpu==1.2\n",
    "python -m nltk.downloader punkt stopwords\n",
    "```\n",
    "\n",
    "To make this environment available in a Linux DLVM JupyterHub:\n",
    "\n",
    "```\n",
    "source activate <myenv>\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --name docqa --display-name \"docqa\"\n",
    "sudo \"/home/<user>/.conda/envs/<myenv>/bin/python\" -m ipykernel install --name <docqa> --display-name \"<docqa>\"\n",
    "```\n",
    "\n",
    "To run this notebook, choose the kernel `\"<docqa>\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors\n",
    "The models we train use the common crawl 840 billion token GloVe word vectors from [here](https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "```\n",
    "cd /home/<user>/notebooks/Transfer_Learning_Text/Transfer_Learning/document-qa/\n",
    "mkdir -p docqa/glove\n",
    "cd docqa/glove\n",
    "wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "unzip glove.840B.300d.zip\n",
    "rm glove.840B.300d.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained Models\n",
    "We have four pre-trained models\n",
    "\n",
    "1. \"squad\" Our model trained on the standard SQuAD dataset, this model is listed on the SQuAD leaderboard \n",
    "as BiDAF + Self Attention\n",
    "\n",
    "2. \"squad-shared-norm\" Our model trained on document-level SQuAD using the shared-norm approach. \n",
    "\n",
    "3. \"triviaqa-web-shared-norm\" Our model trained on TriviaQA web with the shared-norm approach. This \n",
    "is the model we used to submit scores to the TriviaQA leader board.\n",
    " \n",
    "4. \"triviaqa-unfiltered-shared-norm\" Our model trained on TriviaQA unfiltered with the shared-norm approach.\n",
    "This is the model that powers our demo.\n",
    "\n",
    "The models can be downloaded [here](https://drive.google.com/open?id=1Hj9WBQHVa__bqoD5RIOPu2qDpvfJQwjR)\n",
    "\n",
    "The models use the cuDNN implementation of GRUs by default, which means they can only be run on\n",
    "the GPU. We also have much slower, but CPU compatible, versions [here](https://drive.google.com/open?id=1NRmb2YilnZOfyKULUnL7gu3HE5nT0sMy).\n",
    "\n",
    "Once the models is downloaded, store them in:\n",
    "\n",
    "```\n",
    "\n",
    "cd /home/<user>/notebooks/Transfer_Learning_Text/Transfer_Learning/document-qa/\n",
    "mkdir -p  pretrained_models\n",
    "mkdir -p models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing from Linux DLVM Terminal\n",
    "\n",
    "### User Input\n",
    "\"docqa/scripts/1_run_on_user_documents.py\" serves as a heavily commented example of how to run our models \n",
    "and pre-processing pipeline on other kinds of text. For example:\n",
    " \n",
    " ```\n",
    " \n",
    " source activate <myenv>\n",
    " cd /home/<user>/notebooks/Transfer_Learning_Text/Transfer_Learning/document-qa/\n",
    " python docqa/scripts/1_run_on_user_documents.py /path/to/model/directory \n",
    " \"Who wrote the satirical essay 'A Modest Proposal'?\"  \n",
    " ~/data/triviaqa/evidence/wikipedia/A_Modest_Proposal.txt \n",
    " ~/data/triviaqa/evidence/wikipedia/Jonathan_Swift.txt\n",
    " ```\n",
    " \n",
    "### Demo on static document\n",
    "You may try any question by keeping the paragraph content constant. Here we use Harry Shum's Book \"Future Computed\" as out static document. \n",
    "`python docqa/scripts/2_run_on_static_documents.py /path/to/model/directory \n",
    " \"What is AI Law?\"`\n",
    "\n",
    "### Bot-Like Experience on static document\n",
    "You may create a bot using your own paragraph/document. We use the triviaqa-unfiltered-shared-norm model for this test. Here we use Harry Shum's Book \"Future Computed\" as our static document. \n",
    "Now, we need to operationalize the model on this document. We use python Flask API to operationalize the model locally.\n",
    "```\n",
    "python 3_run_flask_api_local_on_static_documents.py \n",
    "```\n",
    "This operationalizes the model at port 5000. To test the bot locally we can run:\n",
    "```\n",
    "python 4_local_static_request.py  \"What is AI Law\"?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing from this Notebook\n",
    "\n",
    "### Bot-Like Experience on Future Computed Book\n",
    "\n",
    "You may create a bot using your own paragraph/document. We use the triviaqa-unfiltered-shared-norm model for this test. Here we use Harry Shum's Book \"Future Computed\" as our static document. \n",
    "\n",
    "Now, we need to operationalize the model on this document. We use python Flask API to operationalize the model locally.\n",
    "\n",
    "In JupyterHub, click on \"New\" button on top-right and open a \"Terminal\". Inside the JupyterHyub \"Terminal\":\n",
    "\n",
    "```\n",
    "cd notebooks/document-qa\n",
    "\"/home/<user>/.conda/envs/<myenv>/bin/python\" docqa/scripts/3_run_flask_api_local_on_static_documents.py\n",
    "```\n",
    "This operationalizes the model locally on DLVM at port 5000. And the flask server is running at JupyterHub Terminal.\n",
    "\n",
    "\n",
    "To test the model locally we can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI law will emerge\r\n"
     ]
    }
   ],
   "source": [
    "!\"/home/antriv/.conda/envs/tf_1.2_py35/bin/python\" docqa/scripts/4_local_static_request.py \"what is AI Law\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go on changing the question and test the model for different questions from this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docqa",
   "language": "python",
   "name": "docqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
